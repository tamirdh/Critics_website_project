{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image captioning deep learning workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNBJF8NEVicXaSv09tmo3RB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirdh/Critics_website_project/blob/master/image_captioning_deep_learning_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ_DSjOA83Oe"
      },
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkLieRxMxqdZ",
        "outputId": "52eeedfe-a1f0-4384-be05-0443d921fce4"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf coco\n",
        "!rm -rf sample_data\n",
        "!mkdir coco\n",
        "%cd coco\n",
        "!mkdir images\n",
        "%cd images\n",
        "\n",
        "!wget -c http://images.cocodataset.org/zips/train2017.zip\n",
        "#!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
        "\n",
        "!unzip train2017.zip\n",
        "#!unzip val2017.zip\n",
        "\n",
        "!rm train2017.zip\n",
        "#!rm val2017.zip\n",
        "\n",
        "%cd ../\n",
        "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "!unzip annotations_trainval2017.zip\n",
        "\n",
        "!rm annotations_trainval2017.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/coco\n",
            "/content/coco/images\n",
            "--2021-06-01 10:47:44--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.36.124\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.36.124|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip’\n",
            "\n",
            "train2017.zip        18%[==>                 ]   3.41G  15.0MB/s    eta 16m 19s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEZMYzlIxCwV"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnc6rR2P-ck-"
      },
      "source": [
        "## Imports and Vocabulary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YUFskmKSJlD"
      },
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "spacy_eng = spacy.load(\"en\")\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self,freq_threshold):\n",
        "        #setting the pre-reserved tokens int to string tokens\n",
        "        # PAD- padding symbol\n",
        "        # SOS- Start of Sentence\n",
        "        # EOS- end of sentence\n",
        "        # UNK- unknown word (unknown\\ below threshold)\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "        #string to int tokens\n",
        "        #its reverse dict self.itos\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}\n",
        "        self.freq_threshold = freq_threshold\n",
        "        \n",
        "    def __len__(self):\n",
        "      return len(self.itos)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "    \n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "                \n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    if idx > 0 and idx % 1000==0:\n",
        "                        print(f\"Added {idx} words to vocab\")\n",
        "                    idx += 1\n",
        "            \n",
        "\n",
        "        print(f\"Done, added {idx-1} words to vocabulary\")\n",
        "    \n",
        "    def numericalize(self,text):\n",
        "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        result = [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkgLLalGAHXn"
      },
      "source": [
        "## Dataset custom class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y19oWTAsWmEC"
      },
      "source": [
        "import pickle\n",
        "class COCODataset(Dataset):\n",
        "    \"\"\"\n",
        "    COCODataset\n",
        "    \"\"\"\n",
        "    def __init__(self,root_dir,annotation_file,transform=None,freq_threshold=5,\n",
        "                 load_vocab=False, vocab_loc = \"vocab.pkl\"):\n",
        "      \"\"\"\n",
        "      can use load_vocab to use a previously created vocabulary (time saving feature)\n",
        "      freq_threshold: words with a count below this number will be marked as <UNK>\n",
        "      \"\"\"\n",
        "      self.root_dir = root_dir\n",
        "      self.coco = COCO(annotation_file)\n",
        "      self.transform = transform\n",
        "      self.cap_max_size = 0\n",
        "      #Get image and caption colum from the dataframe\n",
        "      self.imgs = []\n",
        "      self.captions = []\n",
        "      for idx,ann in enumerate(self.coco.anns.values()):\n",
        "        self.imgs.append(self.coco.loadImgs((ann['image_id']))[0][\"file_name\"])\n",
        "        self.captions.append(ann['caption'])\n",
        "        if (idx) % 1000 == 0 and idx>0:\n",
        "          print(f\"Processed {idx} images and captions\")\n",
        "      print(\"Finished processing images and captions\")\n",
        "      print(f\"Got:{len(set(self.imgs))} pictures with {len(self.captions)} captions!\")\n",
        "      \n",
        "      #Initialize vocabulary and build vocab\n",
        "      if load_vocab:\n",
        "        with open(vocab_loc, \"rb\") as source:\n",
        "          self.vocab = pickle.load(source)\n",
        "        print(f\"Loaded vocabulary from {vocab_loc}\")\n",
        "      \n",
        "      else:\n",
        "        print(\"Build vocabulary\")\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions)\n",
        "        print(\"Finished building vocabulary\")\n",
        "        with open(vocab_loc, \"wb\") as dest:\n",
        "          pickle.dump(self.vocab, dest)\n",
        "      \n",
        "      print(f\"Using {len(self.vocab)} words\")\n",
        "    \n",
        "    def __len__(self):\n",
        "      return len(self.imgs)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "      caption = self.captions[idx]\n",
        "      img_name = self.imgs[idx]\n",
        "      img_location = os.path.join(self.root_dir,img_name)\n",
        "      img = Image.open(img_location).convert(\"RGB\")\n",
        "      \n",
        "      #apply the transfromation to the image\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "      \n",
        "      #numericalize the caption text\n",
        "      caption_vec = [self.vocab.stoi[\"<SOS>\"]]\n",
        "      caption_vec.extend(self.vocab.numericalize(caption))\n",
        "      caption_vec.append(self.vocab.stoi[\"<EOS>\"])\n",
        "      \n",
        "      return img, torch.tensor(caption_vec,dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDqCBk-KCBkw"
      },
      "source": [
        "## Dataloader creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNHTCZ8ZcXGQ"
      },
      "source": [
        "# define a transformation to add some noise and variance to our images\n",
        "transformation = transforms.Compose([transforms.Resize((512,512), Image.NEAREST),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.RandomInvert(),\n",
        "                                     transforms.RandomVerticalFlip(),\n",
        "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                                      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_flS7fBeC0r"
      },
      "source": [
        "class CapsCollate:\n",
        "    \"\"\"\n",
        "    Collate to apply the padding to the captions with dataloader\n",
        "    \"\"\"\n",
        "    def __init__(self,pad_idx,batch_first=False, vec_len=-1):\n",
        "        self.pad_idx = pad_idx\n",
        "        self.batch_first = batch_first\n",
        "        self.vec_len = vec_len + 2 # adding the <SOS> and <EOS>\n",
        "        assert self.vec_len > 0, \"Vector length must be positive integer\"\n",
        "    \n",
        "    def __call__(self,batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs,dim=0)\n",
        "        targets_list = list()\n",
        "        for item in batch:\n",
        "            # item = (img:Image, caption:tensor)\n",
        "            addition = self.vec_len-len(item[1])\n",
        "            padded_target = torch.cat((item[1], torch.empty(addition,dtype=torch.long).fill_(pad_idx)),dim=0)\n",
        "            targets_list.append(padded_target)\n",
        "            #print(f\"GOT:{item[1]}, {item[1].type()}\\nAdding:{addition}\\nPADDED:{padded_target}\\n{padded_target.type()}\")\n",
        "        targets = torch.stack(targets_list,0)\n",
        "        #print(f\"Targets shape:{targets.shape}\")\n",
        "        return imgs,targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igD_-Qs7oFTz"
      },
      "source": [
        "#writing the dataloader\n",
        "#setting the constants\n",
        "BATCH_SIZE = 4\n",
        "NUM_WORKER = 1\n",
        "\n",
        "dataset =  COCODataset(\n",
        "    root_dir = \"/content/coco/images/train2017\",\n",
        "    annotation_file= \"/content/coco/annotations/captions_train2017.json\",\n",
        "    transform=transformation,\n",
        "    freq_threshold=5,\n",
        "    load_vocab=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1ptid0sedWA"
      },
      "source": [
        "\n",
        "#token to represent the padding\n",
        "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=True,\n",
        "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True, vec_len=50)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHHvCoTVl9ge"
      },
      "source": [
        "for i,j in data_loader:\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXIFRNd6Rbih"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxeR_Dg0JnNQ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, train_CNN=False):\n",
        "      super(EncoderCNN, self).__init__()\n",
        "      self.train_CNN = train_CNN\n",
        "      self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
        "      self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, images):\n",
        "      \n",
        "      features = self.inception(images)\n",
        "      output = self.dropout(self.relu(features))\n",
        "      return output\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Input is a CNN network, output will be a caption.\n",
        "  TODO: Check how to implement a transformer for better results\n",
        "  \"\"\"\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "      super(DecoderRNN, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "      self.lstm_cell = nn.LSTMCell(embed_size, hidden_size)\n",
        "      self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "        # batch size\n",
        "        batch_size = features.size(0)\n",
        "        \n",
        "        \n",
        "        # init the hidden and cell states to zeros\n",
        "        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
        "        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
        "    \n",
        "        # define the output tensor placeholder\n",
        "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)\n",
        "\n",
        "        # embed the captions\n",
        "        captions_embed = self.embed(captions)\n",
        "        # tensor of shape (B, LEN, EMBED SIZE)\n",
        "        # LEN- vectors length (longest caption+2)\n",
        "        \n",
        "        # pass the caption word by word\n",
        "        for t in range(captions.size(1)):\n",
        "\n",
        "            # for the first time step the input is the feature vector\n",
        "            if t == 0:\n",
        "                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
        "                \n",
        "            # for the 2nd+ time step, using teacher forcer\n",
        "            else:\n",
        "                #hidden_state, cell_state = self.lstm_cell()\n",
        "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
        "            \n",
        "            # output of the attention mechanism\n",
        "            out = self.fc_out(self.dropout(hidden_state))\n",
        "            # build the output tensor\n",
        "            outputs[:, t, :] = out\n",
        "        return outputs\n",
        "\n",
        "  \n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, train_CNN=False):\n",
        "      super(CNNtoRNN, self).__init__()\n",
        "      self.encoderCNN = EncoderCNN(embed_size, train_CNN).to(device)\n",
        "      self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, images, captions):\n",
        "      features = self.encoderCNN(images)\n",
        "      outputs = self.decoderRNN(features, captions)\n",
        "      return outputs\n",
        "  def caption_images(self, features, vocab, max_len=50):\n",
        "      # Inference part\n",
        "      # Given the image features generate the captions\n",
        "      # input shape: (3,x,y) where, x,y: image size\n",
        "      # ouput: captions list\n",
        "      self.eval()\n",
        "      with torch.no_grad():\n",
        "        image_pred = self.encoderCNN(features)\n",
        "        batch_size = features.size(0)     \n",
        "        # init the hidden and cell states to zeros\n",
        "        hidden_state = torch.zeros((batch_size, self.decoderRNN.hidden_size)).to(device)\n",
        "        cell_state = torch.zeros((batch_size, self.decoderRNN.hidden_size)).to(device)   \n",
        "        captions_embed = None # embedding of partial caption\n",
        "        #starting input\n",
        "        captions = []\n",
        "        for t in range(max_len):\n",
        "            \n",
        "           # for the first time step the input is the feature vector\n",
        "            if t == 0:\n",
        "                hidden_state, cell_state = self.decoderRNN.lstm_cell(image_pred, (hidden_state, cell_state))    \n",
        "            # for the 2nd+ time step, use previously generated caption\n",
        "            else:\n",
        "                hidden_state, cell_state = self.decoderRNN.lstm_cell(captions_embed, (hidden_state, cell_state))\n",
        "            \n",
        "            # output of the attention mechanism\n",
        "            out = self.decoderRNN.fc_out(self.decoderRNN.dropout(hidden_state))\n",
        "            word_idx = torch.argmax(out).item()\n",
        "            captions.append(word_idx)\n",
        "            if vocab.itos[word_idx] == vocab.stoi[\"<EOS>\"]:\n",
        "              break\n",
        "            captions_embed = self.decoderRNN.embed(torch.argmax(out)).unsqueeze(0)\n",
        "            \n",
        "            \n",
        "            # build the output tensor\n",
        "        print(captions)\n",
        "        #covert the vocab idx to words and return sentence\n",
        "        self.train()\n",
        "        return [vocab.itos[idx] for idx in captions if idx != vocab.stoi[\"<PAD>\"]]\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTBnd6f5RgQd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTIQN7TWqOJ"
      },
      "source": [
        "## training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu_HfkQyRiAP"
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(max_epochs, model):\n",
        "  # Hyperparameters\n",
        "  learning_rate = 3e-4\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  \n",
        "  # init model\n",
        "  model = model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  model.train()\n",
        "     \n",
        "\n",
        "  # start epochs\n",
        "  for epoch in range(max_epochs):\n",
        "    for idx, (img, captions) in tqdm(\n",
        "            enumerate(data_loader), total=len(data_loader), leave=False\n",
        "        ):\n",
        "      img = img.to(device)\n",
        "      captions = captions.to(device).long()\n",
        "      output = model(img, captions).to(device)\n",
        "      loss = criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(loss)\n",
        "      optimizer.step()\n",
        "\n",
        "      if idx == 2:\n",
        "        dataiter = iter(data_loader)\n",
        "        img_show,cap = next(dataiter)\n",
        "        print(f\"Loss {loss.item():.5f}\\n\")\n",
        "        demo_cap = model.caption_images(img_show[0:1].to(device), vocab=dataset.vocab, max_len=30)\n",
        "        demo_cap = ' '.join(demo_cap)\n",
        "        print(\"Predicted\")\n",
        "        show_image(img_show[0],title=demo_cap)\n",
        "        print(\"Original\")\n",
        "        cap = cap[0]\n",
        "        print(cap.long())\n",
        "        demo_cap = ' '.join([dataset.vocab.itos[idx2.item()] for idx2 in cap if idx2.item() != dataset.vocab.stoi[\"<PAD>\"]])\n",
        "        show_image(img_show[0],title=demo_cap, transform=False)\n",
        "        #input(\"Contiue?\")\n",
        "        break\n",
        "      if idx > 20:\n",
        "        break\n",
        "    \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CWQGhuOWvF9"
      },
      "source": [
        "## image function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1APdbNyWw8R"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_image(img, title=None, transform=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    \n",
        "    #unnormalize \n",
        "    if transform:\n",
        "      img[0] = img[0] * 0.229\n",
        "      img[1] = img[1] * 0.224 \n",
        "      img[2] = img[2] * 0.225 \n",
        "      img[0] += 0.485 \n",
        "      img[1] += 0.456 \n",
        "      img[2] += 0.406\n",
        "      \n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    \n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rABCvI85Wx_9"
      },
      "source": [
        "## Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6aG4SBFV7UM"
      },
      "source": [
        "embed_size = 1024\n",
        "hidden_size = 512\n",
        "vocab_size = len(dataset.vocab)\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, train_CNN=False)\n",
        "trained_model = train(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eqf6B56tjS1"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9IKx5N0bwiA"
      },
      "source": [
        "v = Vocabulary(freq_threshold=1)\n",
        "\n",
        "v.build_vocab([\"This is a good place to find a city\"])\n",
        "print(v.stoi)\n",
        "print(v.numericalize(\"This is a good place to find a city here!!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC0fwJuEnefW"
      },
      "source": [
        "for img,caption in data_loader:\n",
        "  print(img.size())\n",
        "  captions = caption.unsqueeze(2)\n",
        "  print(captions.shape)\n",
        "  cap = caption[0]\n",
        "  print(cap.size())\n",
        "  print(cap)\n",
        "  print([idx for idx in cap])\n",
        "  print(dataset.vocab.itos[cap[2].item()])\n",
        "  demo_cap = ' '.join([dataset.vocab.itos.get(idx.item(), \"<UNK>\") for idx in cap if idx.item()!=dataset.vocab.stoi[\"<PAD>\"]])\n",
        "  show_image(img[0],title=demo_cap)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r3WAiL0F03A"
      },
      "source": [
        "len(dataset.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6uWkgwGF0yd"
      },
      "source": [
        "a = torch.tensor([1,2])\n",
        "a.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5chs9P76ceJ7"
      },
      "source": [
        "a.unsqueeze(0).unsqueeze(0).shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN6ADBd6oLWk"
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbZYwss2oOeP"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwWCnw_LomY-"
      },
      "source": [
        "dev = xm.xla_device()\n",
        "dev"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}